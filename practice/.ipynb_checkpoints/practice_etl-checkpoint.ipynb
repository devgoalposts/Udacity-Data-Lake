{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('../dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] =     config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "# os.environ[\"AWS_SESSION_TOKEN\"] =     config['AWS']['AWS_SESSION_TOKEN']\n",
    "# os.environ['AWS_DEFAULT_REGION'] =    \"us-east-1\"#config['AWS']['AWS_DEFAULT_REGION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .master(\"spark://pop-os.localdomain:7077\")\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "config = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "config.set(\"fs.s3a.endpoint\", \"s3.us-west-2.amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_events = spark.read.json(\"s3a://udacity-dend/log_data/2018/11/2018-11-05-events.json\")\n",
    "df_events = df_events = df_events.withColumn(\"timestamp\", F.to_timestamp(F.from_unixtime(F.col(\"ts\") / 1000)) )\n",
    "df_songs = spark.read.json(\"s3a://udacity-dend/song_data/A/B/Q/TRABQTA128F148D048.json\")\n",
    "df_events.createOrReplaceTempView(\"staging_events\")\n",
    "df_songs.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_events = spark.read.json(\"s3a://udacity-dend/log_data/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"select * from staging_events\").limit(5).toPandas()\n",
    "df_events = df_events = df_events.withColumn(\"timestamp\", F.to_timestamp(F.from_unixtime(F.col(\"ts\") / 1000)) )\n",
    "df_events.createOrReplaceTempView(\"staging_events\")\n",
    "df_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+-------------------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|          timestamp|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+-------------------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|2018-11-14 19:30:26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|2018-11-14 19:41:21|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|2018-11-14 19:45:41|\n",
      "|       null|Logged In|    Wyatt|     M|            0|   Scott|     null| free|Eureka-Arcata-For...|   GET|    Home|1.540872073796E12|      563|           null|   200|1542247071796|Mozilla/5.0 (Wind...|     9|2018-11-14 20:57:51|\n",
      "|       null|Logged In|   Austin|     M|            0| Rosales|     null| free|New York-Newark-J...|   GET|    Home|1.541059521796E12|      521|           null|   200|1542252577796|Mozilla/5.0 (Wind...|    12|2018-11-14 22:29:37|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8056"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.where(F.col(\"page\") == \"NextSong\").sort(F.col(\"song\").asc()).limit(100).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_songs = spark.read.json(\"s3a://udacity-dend/song_data/*/*/*/*.json\")\n",
    "# df_songs.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_songs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from staging_songs\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "songplays_schema = StructType(\n",
    "    [\n",
    "        StructField(\"songplay_id\", StringType(), True),\n",
    "        StructField(\"start_time\", DoubleType(), True),\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"level\", StringType(), True),\n",
    "        StructField(\"song_id\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"session_id\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"user_agent\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "df_songplays = spark.createDataFrame([], songplays_schema)\n",
    "df_songplays.createOrReplaceTempView(\"songplays\")\n",
    "\n",
    "\n",
    "users_schema = StructType(\n",
    "    [\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"level\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "df_users = spark.createDataFrame([], songplays_schema)\n",
    "df_users.createOrReplaceTempView(\"users\")\n",
    "\n",
    "\n",
    "songs_schema = StructType(\n",
    "    [\n",
    "        StructField(\"song_id\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"year\", ShortType(), True),\n",
    "        StructField(\"duration\", DoubleType(), True)\n",
    "    ]\n",
    ")\n",
    "df_songs = spark.createDataFrame([], songs_schema)\n",
    "df_songs.createOrReplaceTempView(\"songs\")\n",
    "\n",
    "\n",
    "artists_schema = StructType(\n",
    "    [\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True)\n",
    "    ]\n",
    ")\n",
    "df_artists = spark.createDataFrame([], artists_schema)\n",
    "df_artists.createOrReplaceTempView(\"artists\")\n",
    "\n",
    "\n",
    "time_schema = StructType(\n",
    "    [\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"day\", IntegerType(), True),\n",
    "        StructField(\"month\", IntegerType(), True),\n",
    "        StructField(\"year\", ShortType(), True),\n",
    "        StructField(\"weekend\", BooleanType(), True)\n",
    "    ]\n",
    ")\n",
    "df_time = spark.createDataFrame([], time_schema)\n",
    "df_time.createOrReplaceTempView(\"artists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub_frame = df_events[[\n",
    "    \"itemInSession\", \n",
    "    \"ts\", \n",
    "    \"userId\", \n",
    "    \"level\",\n",
    "    \"artist\",\n",
    "    \"song\",\n",
    "    \"sessionId\", \n",
    "    \"location\", \n",
    "    \"userAgent\"\n",
    "]]# .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events[[\"ts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_frame = df_songplays.union(sub_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_frame.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from songs\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## song table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(song_id), title, artist_id, year, duration \n",
    "    FROM staging_songs\n",
    "\"\"\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## artist table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(artist_id), artist_name, artist_location, artist_latitude, artist_longitude\n",
    "    FROM staging_songs\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT(userId) as user_id, firstName, lastName, gender, level\n",
    "    FROM staging_events\n",
    "    WHERE page ='NextSong'\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "added_timestamp = df_events.withColumn(\"timestamp\", F.to_timestamp(F.from_unixtime(F.col(\"ts\") / 1000)) )\n",
    "added_timestamp.createOrReplaceTempView(\"staging_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_weekday(x):\n",
    "    if x == 7 or x == 1:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "import datetime\n",
    "added_timestamp.select(\n",
    "    F.col(\"timestamp\").alias('start_time'),\n",
    "    F.hour(\"timestamp\").alias('hour'),\n",
    "    F.dayofmonth(\"timestamp\").alias('day'),\n",
    "    F.weekofyear(\"timestamp\").alias('week'),\n",
    "    F.month(\"timestamp\").alias('month'), \n",
    "    F.year(\"timestamp\").alias('year'), \n",
    "    # F.dayofweek(\"timestamp\").alias('day of week'),\n",
    "    F.when(  F.dayofweek(\"timestamp\") ==  7 , False).when(  F.dayofweek(\"timestamp\") ==  1 , False).otherwise(True).alias(\"weekday\")\n",
    ").limit(5).toPandas()# .where(added_timestamp.timestamp > datetime.datetime(2018, 11, 5 )).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "data=[[\"-1\",\"2018-11-03\"], [\"0\",\"2018-11-04\"], [\"1\",\"2018-11-05\"],[\"2\",\"2018-11-06\"],[\"3\",\"2018-11-07\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df = df.withColumn(\"timestamp\", F.to_date(F.col(\"input\"), \"yyyy-MM-dd\") )\n",
    "df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(\"timestamp\") ) )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df.select(F.col(\"input\"), \n",
    "#     F.to_date(F.col(\"input\"), \"yyyy-MM-dd\").alias(\"date_format\"),\n",
    "#     F.dayofweek(\"date_format\").alias('day of week')\n",
    "#   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambda x : True if (x == 7 or x == 1) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# SELECT timestamp FROM staging_events as ts,\n",
    "#     EXTRACT(hour FROM staged_date)\n",
    "# \"\"\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_events.select(F.to_timestamp(F.from_unixtime( F.col(\"ts\") / 1000 )).alias('time_stamp'))\n",
    "# df_songs = spark.read.json(\"s3a://udacity-dend/song_data/A/B/Q/TRABQTA128F148D048.json\")\n",
    "#added_timestamp = df_events.withColumn(\"unix_timestamp\", F.col(\"unix_timestamp_int\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events[[\"userId\", \"level\", \"song\"]].toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## songplay table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT st.userId,\n",
    "    st.level,\n",
    "    s.song_id,\n",
    "    s.artist_id,\n",
    "    st.sessionId,\n",
    "    st.location,\n",
    "    st.userAgent\n",
    "FROM staging_events st \n",
    "INNER JOIN staging_songs s ON s.title=st.song AND st.artist = s.artist_name\n",
    "WHERE st.page = 'NextSong'\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events[[\"userId\", \"level\", \"song_id\", \"artist_id\", \"sessionId\", \"location\", \"userAgent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM staging_events\n",
    "WHERE page='NextSong'\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songs_final_df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM staging_events\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.toJSON(\"./test.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.coalesce(1).write.format('json').save('./test.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_events.write.parquet(\"s3a://paul-spark/songs.parquet\",mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
